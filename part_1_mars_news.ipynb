{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12 Challenge\n",
    "## Deliverable 1: Scrape Titles and Preview Text from Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "DriverNotFoundError",
     "evalue": "Driver for chrome was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDriverNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Initialize the browser (assumes you have chromedriver installed and in your path)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m browser \u001b[39m=\u001b[39m Browser(\u001b[39m'\u001b[39;49m\u001b[39mchrome\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\splinter\\browser.py:123\u001b[0m, in \u001b[0;36mBrowser\u001b[1;34m(driver_name, retry_count, *args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[39mraise\u001b[39;00m DriverNotFoundError(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdriver_name\u001b[39m}\u001b[39;00m\u001b[39m is not a recognized driver.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m driver \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mraise\u001b[39;00m DriverNotFoundError(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDriver for \u001b[39m\u001b[39m{\u001b[39;00mdriver_name\u001b[39m}\u001b[39;00m\u001b[39m was not found.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m get_driver(driver, retry_count\u001b[39m=\u001b[39mretry_count, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mDriverNotFoundError\u001b[0m: Driver for chrome was not found."
     ]
    }
   ],
   "source": [
    "# Initialize the browser (assumes you have chromedriver installed and in your path)\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Visit the Website\n",
    "\n",
    "1. Use automated browsing to visit the [Mars news site](https://static.bc-edx.com/data/web/mars_news/index.html). Inspect the page to identify which elements to scrape.\n",
    "\n",
    "      > **Hint** To identify which elements to scrape, you might want to inspect the page by using Chrome DevTools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'browser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Visit the Mars news site\u001b[39;00m\n\u001b[0;32m      2\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://static.bc-edx.com/data/web/mars_news/index.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m browser\u001b[39m.\u001b[39mvisit(url)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'browser' is not defined"
     ]
    }
   ],
   "source": [
    "# Visit the Mars news site\n",
    "url = \"https://static.bc-edx.com/data/web/mars_news/index.html\"\n",
    "browser.visit(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the Website\n",
    "\n",
    "Create a Beautiful Soup object and use it to extract text elements from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'browser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Parse the page using Beautiful Soup\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m html \u001b[39m=\u001b[39m browser\u001b[39m.\u001b[39mhtml\n\u001b[0;32m      3\u001b[0m soup \u001b[39m=\u001b[39m soup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Initialize an empty list to store the dictionaries\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'browser' is not defined"
     ]
    }
   ],
   "source": [
    "# Parse the page using Beautiful Soup\n",
    "html = browser.html\n",
    "soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'descendants'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Extract all the text elements\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m text_elements \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39;49m\u001b[39mdiv\u001b[39;49m\u001b[39m'\u001b[39;49m, class_\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlist_text\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\CDiFa\\.conda\\envs\\dev\\lib\\site-packages\\bs4\\element.py:2026\u001b[0m, in \u001b[0;36mTag.find_all\u001b[1;34m(self, name, attrs, recursive, string, limit, **kwargs)\u001b[0m\n\u001b[0;32m   2008\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_all\u001b[39m(\u001b[39mself\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attrs\u001b[39m=\u001b[39m{}, recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, string\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2009\u001b[0m              limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2010\u001b[0m     \u001b[39m\"\"\"Look in the children of this PageElement and find all\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[39m    PageElements that match the given criteria.\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2024\u001b[0m \u001b[39m    :rtype: bs4.element.ResultSet\u001b[39;00m\n\u001b[0;32m   2025\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2026\u001b[0m     generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdescendants\n\u001b[0;32m   2027\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m recursive:\n\u001b[0;32m   2028\u001b[0m         generator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'descendants'"
     ]
    }
   ],
   "source": [
    "# Extract all the text elements\n",
    "text_elements = soup.find_all('div', class_='list_text')\n",
    "#print(text_elements)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Store the Results\n",
    "\n",
    "Extract the titles and preview text of the news articles that you scraped. Store the scraping results in Python data structures as follows:\n",
    "\n",
    "* Store each title-and-preview pair in a Python dictionary. And, give each dictionary two keys: `title` and `preview`. An example is the following:\n",
    "\n",
    "  ```python\n",
    "  {'title': \"NASA's MAVEN Observes Martian Light Show Caused by Major Solar Storm\", \n",
    "   'preview': \"For the first time in its eight years orbiting Mars, NASA’s MAVEN mission witnessed two different types of ultraviolet aurorae simultaneously, the result of solar storms that began on Aug. 27.\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "* Store all the dictionaries in a Python list.\n",
    "\n",
    "* Print the list in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the dictionaries\n",
    "mars_news_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape titles and preview text\n",
    "for article in soup.find_all('div', class_='article'):\n",
    "    title = article.find('h2').text  # Extracting the title\n",
    "    preview = article.find('p').text  # Extracting the preview text\n",
    "    \n",
    "    # Storing the title and preview as a dictionary\n",
    "    mars_news = {\n",
    "        \"title\": title,\n",
    "        \"preview\": preview\n",
    "    }\n",
    "    \n",
    "    # Adding the dictionary to the list\n",
    "    mars_news_list.append(mars_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': \"NASA's MAVEN Observes Martian Light Show Caused by Major Solar Storm\",\n",
       "  'preview': 'For the first time in its eight years orbiting Mars, NASA’s MAVEN mission witnessed two different types of ultraviolet aurorae simultaneously, the result of solar storms that began on Aug. 27.'},\n",
       " {'title': \"NASA Prepares to Say 'Farewell' to InSight Spacecraft\",\n",
       "  'preview': 'A closer look at what goes into wrapping up the mission as the spacecraft’s power supply continues to dwindle.'},\n",
       " {'title': 'NASA and ESA Agree on Next Steps to Return Mars Samples to Earth',\n",
       "  'preview': 'The agency’s Perseverance rover will establish the first sample depot on Mars.'},\n",
       " {'title': \"NASA's InSight Lander Detects Stunning Meteoroid Impact on Mars\",\n",
       "  'preview': 'The agency’s lander felt the ground shake during the impact while cameras aboard the Mars Reconnaissance Orbiter spotted the yawning new crater from space.'},\n",
       " {'title': 'NASA To Host Briefing on InSight, Mars Reconnaissance Orbiter Findings',\n",
       "  'preview': 'Scientists from two Mars missions will discuss how they combined images and data for a major finding on the Red Planet.'},\n",
       " {'title': 'Why NASA Is Trying To Crash Land on Mars',\n",
       "  'preview': 'Like a car’s crumple zone, the experimental SHIELD lander is designed to absorb a hard impact.'},\n",
       " {'title': 'Curiosity Mars Rover Reaches Long-Awaited Salty Region',\n",
       "  'preview': 'After years of climbing, the Mars rover has arrived at a special region believed to have formed as Mars’ climate was drying.'},\n",
       " {'title': 'Mars Mission Shields Up for Tests',\n",
       "  'preview': 'Protecting Mars Sample Return spacecraft from micrometeorites requires high-caliber work.'},\n",
       " {'title': \"NASA's InSight Waits Out Dust Storm\",\n",
       "  'preview': 'InSight’s team is taking steps to help the solar-powered lander continue operating for as long as possible.'},\n",
       " {'title': \"NASA's InSight 'Hears' Its First Meteoroid Impacts on Mars\",\n",
       "  'preview': 'The Mars lander’s seismometer has picked up vibrations from four separate impacts in the past two years.'},\n",
       " {'title': \"NASA's Perseverance Rover Investigates Geologically Rich Mars Terrain\",\n",
       "  'preview': 'The latest findings provide greater detail on a region of the Red Planet that has a watery past and is yielding promising samples for the NASA-ESA Mars Sample Return campaign.'},\n",
       " {'title': 'NASA to Host Briefing on Perseverance Mars Rover Mission Operations',\n",
       "  'preview': 'Members of the mission will discuss the rover’s activities as it gathers samples in an ancient river delta.'},\n",
       " {'title': \"NASA's Perseverance Makes New Discoveries in Mars' Jezero Crater\",\n",
       "  'preview': 'The rover found that Jezero Crater’s floor is made up of volcanic rocks that have interacted with water.'},\n",
       " {'title': \"10 Years Since Landing, NASA's Curiosity Mars Rover Still Has Drive\",\n",
       "  'preview': 'Despite signs of wear, the intrepid spacecraft is about to start an exciting new chapter of its mission as it climbs a Martian mountain.'},\n",
       " {'title': \"SAM's Top 5 Discoveries Aboard NASA's Curiosity Rover at Mars\",\n",
       "  'preview': '“Selfie” of the Curiosity rover with inset showing the SAM instrument prior to installation on the rover.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the list in the notebook\n",
    "print(mars_news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
